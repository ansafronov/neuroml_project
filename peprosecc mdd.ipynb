{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6d33b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% Import Libraries\n",
    "import mne\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from scipy.io import savemat\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ae64f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/MDD/MDD S21 EO.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Not setting metadata\n",
      "107 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 107 events and 3001 original time points ...\n",
      "6 bad epochs dropped\n",
      "0\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/MDD/MDD S8 TASK.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Not setting metadata\n",
      "227 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 227 events and 3001 original time points ...\n",
      "6 bad epochs dropped\n",
      "1\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/MDD/MDD S10 EO.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Not setting metadata\n",
      "107 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 107 events and 3001 original time points ...\n",
      "6 bad epochs dropped\n",
      "2\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/MDD/MDD S13 EC.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Not setting metadata\n",
      "107 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 107 events and 3001 original time points ...\n",
      "6 bad epochs dropped\n",
      "3\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/MDD/MDD S2  EC.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Not setting metadata\n",
      "95 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 95 events and 3001 original time points ...\n",
      "6 bad epochs dropped\n",
      "4\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/MDD/MDD S18 EO.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Not setting metadata\n",
      "107 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 107 events and 3001 original time points ...\n",
      "6 bad epochs dropped\n",
      "5\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/MDD/MDD S24  EC.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Not setting metadata\n",
      "107 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 107 events and 3001 original time points ...\n",
      "6 bad epochs dropped\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 52\u001b[0m\n\u001b[1;32m     50\u001b[0m count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(np\u001b[38;5;241m.\u001b[39mmin(epochs),np\u001b[38;5;241m.\u001b[39mmax(epochs)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 52\u001b[0m     vals \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;28mlist\u001b[39m(np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(epochs))[\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m])])\u001b[38;5;241m.\u001b[39mtranspose()[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     53\u001b[0m     vals \u001b[38;5;241m=\u001b[39m vals[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m,:\u001b[38;5;241m30\u001b[39m\u001b[38;5;241m*\u001b[39mtarget_fs]\n\u001b[1;32m     54\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m+\u001b[39m epoch \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mmin(epochs):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%% Load, Normalize, and Segment Data (Assumes that HC and MDD Data are in Separate Directories)\n",
    "base = os.getcwd()\n",
    "folderpath1 = base + \"/HC/\"\n",
    "folderpath2 = base + \"/MDD/\"\n",
    "\n",
    "target_fs = 100\n",
    "\n",
    "for f in range(1,2):\n",
    "    if f == 0:\n",
    "        folderpath = folderpath1\n",
    "    else:\n",
    "        folderpath = folderpath2\n",
    "        \n",
    "    # Find Files\n",
    "    files = os.listdir(folderpath)\n",
    "    \n",
    "    # Load Files\n",
    "    subject = []\n",
    "    data = []\n",
    "    for i in tqdm(range(0,len(files)), total=len(files)):\n",
    "        file=mne.io.read_raw_edf(os.path.join(folderpath,files[i])).resample(sfreq=target_fs)\n",
    "        data_len = np.shape(file.times)[0]\n",
    "        nepochs = int(np.floor(data_len/(target_fs*30))-1)\n",
    "        events = mne.make_fixed_length_events(file, start=0, stop=nepochs*30-1, duration=2.5) # make events every 2.5 seconds\n",
    "        nepochs = np.shape(events)[0]\n",
    "        # epoch_file =  mne.Epochs(file, events, tmin=0, tmax=5,baseline=None)\n",
    "        epoch_file =  mne.Epochs(file, events, tmin=-15.0, tmax=15.0,baseline=None) # When making epochs, define them as 12.5 seconds before to 12.5 seconds after previously defined events\n",
    "        file = []\n",
    "        \n",
    "        df=epoch_file.to_data_frame()\n",
    "        epoch_file = []\n",
    "        channels = df.columns[3:]\n",
    "        channels_to_use = ['EEG Fp1-LE', 'EEG Fp2-LE', 'EEG F7-LE', 'EEG F3-LE', 'EEG Fz-LE', 'EEG F4-LE', 'EEG F8-LE', 'EEG T3-LE', 'EEG C3-LE', 'EEG Cz-LE', 'EEG C4-LE', 'EEG T4-LE', 'EEG T5-LE', 'EEG P3-LE', 'EEG Pz-LE', 'EEG P4-LE', 'EEG T6-LE', 'EEG O1-LE', 'EEG O2-LE']\n",
    "        \n",
    "        epochs = df.epoch\n",
    "        df=df[channels_to_use]\n",
    "    \n",
    "        # Z-Score Each Channel\n",
    "        mean_vals = df.mean(axis=0)\n",
    "        sd_vals = df.std(axis=0)\n",
    "        for ch in channels_to_use:\n",
    "            df[ch] -= mean_vals[ch]\n",
    "            df[ch] /= sd_vals[ch]\n",
    "        \n",
    "        if f == 0:\n",
    "            subj = int(int(files[i][3:5])+f*100) # NC subject numbers are their actual numbers, MDD start from 100 + their subject number\n",
    "        else:\n",
    "            subj = int(int(files[i][5:7])+f*100) # NC subject numbers are their actual numbers, MDD start from 100 + their subject number\n",
    "\n",
    "        count = 0\n",
    "        for epoch in range(np.min(epochs),np.max(epochs)+1):\n",
    "            vals = np.array(df.iloc[list(np.arange(len(epochs))[list(epoch*np.ones_like(epochs)==epochs)])]).transpose()[None]\n",
    "            vals = vals[...,:30*target_fs]\n",
    "            if i + epoch == np.min(epochs):\n",
    "                data = list(vals)\n",
    "            else:\n",
    "                data = np.append(data,vals,axis=0)\n",
    "            count+=1\n",
    "        \n",
    "        df = []\n",
    "        \n",
    "        if i == 0:\n",
    "            subject = list((subj*np.ones((count,))).astype(int))\n",
    "        else:\n",
    "            subject.extend(list((subj*np.ones((count,))).astype(int)))\n",
    "        \n",
    "        print(i)\n",
    "        \n",
    "    if f == 0:\n",
    "        # data_out = data\n",
    "        # subject_out = subject\n",
    "        label = np.zeros_like(subject)\n",
    "        filename1 = base + '/segmented_hc1_data_like_sleep'\n",
    "        filename2 = base + '/segmented_hc2_data_like_sleep'\n",
    "\n",
    "    else:\n",
    "        # data_out = np.concatenate((data_out,data),axis=0)\n",
    "        # subject_out = np.concatenate((subject_out,subject),axis=0)\n",
    "        label = np.ones_like(subject)\n",
    "        filename1 = base + '/segmented_mdd1_data_like_sleep'\n",
    "        filename2 = base + '/segmented_mdd2_data_like_sleep'\n",
    "    \n",
    "    # Split Data into Separate Files for Saving\n",
    "    n_samples_per_file = int(np.floor(len(label)/2))\n",
    "    \n",
    "    save_data1 = {'data':data[:n_samples_per_file,...],'subject':subject[:n_samples_per_file],'channels':channels_to_use,'label':label[:n_samples_per_file]}\n",
    "\n",
    "    np.save(filename1,save_data1)\n",
    "    \n",
    "    save_data1 = [];\n",
    "    \n",
    "    save_data2 = {'data':data[n_samples_per_file:,...],'subject':subject[n_samples_per_file:],'channels':channels_to_use,'label':label[n_samples_per_file:]}\n",
    "\n",
    "    np.save(filename2,save_data2)\n",
    "    \n",
    "    save_data2 = [];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad43ee8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     # Split Data into Separate Files for Saving\n",
    "#     n_samples_per_file = int(np.floor(len(label)/2))\n",
    "    \n",
    "#     save_data1 = {'data':data[:n_samples_per_file,...],'subject':subject[:n_samples_per_file],'channels':channels_to_use,'label':label[:n_samples_per_file]}\n",
    "\n",
    "#     np.save(filename1,save_data1)\n",
    "    \n",
    "#     save_data1 = [];\n",
    "    \n",
    "#     save_data2 = {'data':data[n_samples_per_file:,...],'subject':subject[n_samples_per_file:],'channels':channels_to_use,'label':label[n_samples_per_file:]}\n",
    "\n",
    "#     np.save(filename2,save_data2)\n",
    "    \n",
    "#     save_data2 = [];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe0370a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
