{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import mne\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_path = 'physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/'\n",
    "\n",
    "files = os.listdir(global_path)\n",
    "annot_files = {int(x.split('/')[-1][3:6]): x for x in files if 'Hypnogram' in x}\n",
    "data_files = {int(x.split('/')[-1][3:6]): x for x in files if 'PSG' in x}\n",
    "subj_ids = data_files.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id_global_map = {\n",
    "    'Sleep stage 1': 1,\n",
    "    'Sleep stage 2': 2,\n",
    "    'Sleep stage 3': 3,\n",
    "    'Sleep stage 4': 3, # remap\n",
    "    'Sleep stage ?': 5,\n",
    "    'Sleep stage R': 6,\n",
    "    'Sleep stage W': 7,\n",
    "    'Movement time': 8\n",
    "}\n",
    "\n",
    "stages_to_drop = [5, 8] # drop \"Sleep stage ?\" and \"Movement time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 7750000)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw.get_data().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88b1283546b4925b52cbdd9aebd0320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4001E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153212/3812398702.py:12: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(file_path, preload=True).resample(sfreq=target_fs)\n",
      "/tmp/ipykernel_153212/3812398702.py:12: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(file_path, preload=True).resample(sfreq=target_fs)\n",
      "/tmp/ipykernel_153212/3812398702.py:12: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  raw = mne.io.read_raw_edf(file_path, preload=True).resample(sfreq=target_fs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading 0 ... 7949999  =      0.000 ... 79499.990 secs...\n",
      "Sampling frequency of the instance is already 100.0, returning unmodified.\n",
      "Used Annotations descriptions: ['Sleep stage 1', 'Sleep stage 2', 'Sleep stage 3', 'Sleep stage 4', 'Sleep stage R', 'Sleep stage W']\n",
      "Not setting metadata\n",
      "151 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 151 events and 3001 original time points ...\n",
      "0 bad epochs dropped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153212/3812398702.py:51: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ch] -= mean_vals[ch]\n",
      "/tmp/ipykernel_153212/3812398702.py:52: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[ch] /= sd_vals[ch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4002E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "Reading 0 ... 8489999  =      0.000 ... 84899.990 secs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_153212/3812398702.py:12: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(file_path, preload=True).resample(sfreq=target_fs)\n",
      "/tmp/ipykernel_153212/3812398702.py:12: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  raw = mne.io.read_raw_edf(file_path, preload=True).resample(sfreq=target_fs)\n",
      "/tmp/ipykernel_153212/3812398702.py:12: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  raw = mne.io.read_raw_edf(file_path, preload=True).resample(sfreq=target_fs)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m file_path \u001b[38;5;241m=\u001b[39m global_path \u001b[38;5;241m+\u001b[39m data_files[subj]\n\u001b[1;32m     10\u001b[0m file_path_annot \u001b[38;5;241m=\u001b[39m global_path \u001b[38;5;241m+\u001b[39m annot_files[subj]\n\u001b[0;32m---> 12\u001b[0m raw \u001b[38;5;241m=\u001b[39m \u001b[43mmne\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_raw_edf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mresample(sfreq\u001b[38;5;241m=\u001b[39mtarget_fs)\n\u001b[1;32m     14\u001b[0m start_time \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mtimes[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m end_time \u001b[38;5;241m=\u001b[39m raw\u001b[38;5;241m.\u001b[39mtimes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/edf/edf.py:1689\u001b[0m, in \u001b[0;36mread_raw_edf\u001b[0;34m(input_fname, eog, misc, stim_channel, exclude, infer_types, include, preload, units, encoding, exclude_after_unique, verbose)\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1688\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly EDF files are supported, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mRawEDF\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1690\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_fname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_fname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1691\u001b[0m \u001b[43m    \u001b[49m\u001b[43meog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meog\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1692\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmisc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmisc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1693\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstim_channel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstim_channel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1694\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1695\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfer_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1697\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1698\u001b[0m \u001b[43m    \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1700\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexclude_after_unique\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_after_unique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1701\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1702\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<decorator-gen-203>:12\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, input_fname, eog, misc, stim_channel, exclude, infer_types, preload, include, units, encoding, exclude_after_unique, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/edf/edf.py:187\u001b[0m, in \u001b[0;36mRawEDF.__init__\u001b[0;34m(self, input_fname, eog, misc, stim_channel, exclude, infer_types, preload, include, units, encoding, exclude_after_unique, verbose)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;66;03m# Raw attributes\u001b[39;00m\n\u001b[1;32m    186\u001b[0m last_samps \u001b[38;5;241m=\u001b[39m [edf_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnsamples\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43minput_fname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_extras\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43medf_info\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlast_samps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlast_samps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43morig_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mint\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43morig_units\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_units\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# Read annotations from file and set it\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(edf_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtal_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Read TAL data exploiting the header info (no regexp)\u001b[39;00m\n",
      "File \u001b[0;32m<decorator-gen-185>:12\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self, info, preload, first_samps, last_samps, filenames, raw_extras, orig_format, dtype, buffer_size_sec, orig_units, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/base.py:301\u001b[0m, in \u001b[0;36mBaseRaw.__init__\u001b[0;34m(self, info, preload, first_samps, last_samps, filenames, raw_extras, orig_format, dtype, buffer_size_sec, orig_units, verbose)\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;66;03m# If we have True or a string, actually do the preloading\u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_from_disk:\n\u001b[0;32m--> 301\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_kwargs \u001b[38;5;241m=\u001b[39m _get_argvalues()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/base.py:596\u001b[0m, in \u001b[0;36mBaseRaw._preload_data\u001b[0;34m(self, preload)\u001b[0m\n\u001b[1;32m    591\u001b[0m     data_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    592\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReading \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m ... \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m  =  \u001b[39m\u001b[38;5;132;01m%9.3f\u001b[39;00m\u001b[38;5;124m ... \u001b[39m\u001b[38;5;132;01m%9.3f\u001b[39;00m\u001b[38;5;124m secs...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;241m%\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimes) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    595\u001b[0m )\n\u001b[0;32m--> 596\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_segment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_buffer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_buffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnchan\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    598\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreload \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m<decorator-gen-187>:12\u001b[0m, in \u001b[0;36m_read_segment\u001b[0;34m(self, start, stop, sel, data_buffer, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/base.py:472\u001b[0m, in \u001b[0;36mBaseRaw._read_segment\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# reindex back to original file\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     orig_idx \u001b[38;5;241m=\u001b[39m _convert_slice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_picks[fi][need_idx])\n\u001b[0;32m--> 472\u001b[0m     \u001b[43m_ReadSegmentFileProtector\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_segment_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis_sl\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43morig_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstart_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstop_file\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m     offset \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m n_read\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/base.py:2598\u001b[0m, in \u001b[0;36m_ReadSegmentFileProtector._read_segment_file\u001b[0;34m(self, data, idx, fi, start, stop, cals, mult)\u001b[0m\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_segment_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, idx, fi, start, stop, cals, mult):\n\u001b[0;32m-> 2598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__raw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_segment_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmult\u001b[49m\n\u001b[1;32m   2600\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/edf/edf.py:220\u001b[0m, in \u001b[0;36mRawEDF._read_segment_file\u001b[0;34m(self, data, idx, fi, start, stop, cals, mult)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_segment_file\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, idx, fi, start, stop, cals, mult):\n\u001b[1;32m    219\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read a chunk of raw data.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read_segment_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raw_extras\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filenames\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmult\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/io/edf/edf.py:453\u001b[0m, in \u001b[0;36m_read_segment_file\u001b[0;34m(data, idx, fi, start, stop, raw_extras, filenames, cals, mult)\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m smp_read \u001b[38;5;241m!=\u001b[39m smp_exp:\n\u001b[1;32m    452\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m (ones[i, smp_read:] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mall()  \u001b[38;5;66;03m# sanity check\u001b[39;00m\n\u001b[0;32m--> 453\u001b[0m         ones[i, :] \u001b[38;5;241m=\u001b[39m \u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m            \u001b[49m\u001b[43mones\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43msmp_read\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m            \u001b[49m\u001b[43msmp_exp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    456\u001b[0m \u001b[43m            \u001b[49m\u001b[43msmp_read\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m            \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m         resampled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# give warning if we resampled a subselection\u001b[39;00m\n",
      "File \u001b[0;32m<decorator-gen-54>:12\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(x, up, down, axis, window, n_jobs, pad, npad, method, verbose)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/filter.py:1880\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1878\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(pad\u001b[38;5;241m=\u001b[39mpad, window\u001b[38;5;241m=\u001b[39mwindow, n_jobs\u001b[38;5;241m=\u001b[39mn_jobs)\n\u001b[1;32m   1879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfft\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1880\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43m_resample_fft\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnpad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mratio\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfinal_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1882\u001b[0m     up, down, kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _prep_polyphase(\n\u001b[1;32m   1883\u001b[0m         ratio, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], final_len, window\n\u001b[1;32m   1884\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/filter.py:1976\u001b[0m, in \u001b[0;36m_resample_fft\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1974\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(x_flat), new_len \u001b[38;5;241m-\u001b[39m to_removes\u001b[38;5;241m.\u001b[39msum()), dtype\u001b[38;5;241m=\u001b[39mx_flat\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m   1975\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m xi, x_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(x_flat):\n\u001b[0;32m-> 1976\u001b[0m         y[xi] \u001b[38;5;241m=\u001b[39m \u001b[43m_fft_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnpads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_removes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcuda_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1978\u001b[0m     y \u001b[38;5;241m=\u001b[39m parallel(\n\u001b[1;32m   1979\u001b[0m         p_fun(x_, new_len, npads, to_removes, cuda_dict, pad) \u001b[38;5;28;01mfor\u001b[39;00m x_ \u001b[38;5;129;01min\u001b[39;00m x_flat\n\u001b[1;32m   1980\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/mne/cuda.py:354\u001b[0m, in \u001b[0;36m_fft_resample\u001b[0;34m(x, new_len, npads, to_removes, cuda_dict, pad)\u001b[0m\n\u001b[1;32m    352\u001b[0m     x_fft[nyq : nyq \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m shorter \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.5\u001b[39m\n\u001b[1;32m    353\u001b[0m x_fft \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m cuda_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mW\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 354\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mcuda_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mirfft\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_fft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# now let's trim it back to the correct size (if there was padding)\u001b[39;00m\n\u001b[1;32m    357\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (to_removes \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39many():\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/fft/_backend.py:25\u001b[0m, in \u001b[0;36m_ScipyBackend.__ua_function__\u001b[0;34m(method, args, kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/scipy/fft/_pocketfft/basic.py:97\u001b[0m, in \u001b[0;36mc2r\u001b[0;34m(forward, x, n, axis, norm, overwrite_x, workers, plan)\u001b[0m\n\u001b[1;32m     94\u001b[0m     tmp, _ \u001b[38;5;241m=\u001b[39m _fix_shape_1d(tmp, (n\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, axis)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;66;03m# Note: overwrite_x is not utilized\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpfft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc2r\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = []\n",
    "label = []\n",
    "subject = []\n",
    "\n",
    "target_fs = 100\n",
    "channels_to_use = ['EEG Fpz-Cz']\n",
    "\n",
    "for subj in tqdm(sorted(list(subj_ids))[:100]):\n",
    "    file_path = global_path + data_files[subj]\n",
    "    file_path_annot = global_path + annot_files[subj]\n",
    "\n",
    "    raw = mne.io.read_raw_edf(file_path, preload=True).resample(sfreq=target_fs)\n",
    "    \n",
    "    start_time = raw.times[0]\n",
    "    end_time = raw.times[-1]\n",
    "    trim_start = 1000  # trim 10 seconds from the beginning\n",
    "    trim_end = 1000     # trim 5 seconds from the end\n",
    "    raw = raw.crop(tmin=start_time + trim_start, tmax=end_time - trim_end)\n",
    "    \n",
    "    file_annot = mne.read_annotations(file_path_annot)\n",
    "    offset = 3\n",
    "    delete_annotations = [i for i, x in enumerate(file_annot) if x['description']=='Sleep stage W' and (i < offset or i >= len(file_annot) - offset)]\n",
    "    file_annot.delete(delete_annotations)\n",
    "    delete_annotations = [i for i, x in enumerate(file_annot) if x['description'] in {'Sleep stage ?', 'Movement time'}]\n",
    "    file_annot.delete(delete_annotations)\n",
    "    raw.set_annotations(file_annot)\n",
    "\n",
    "    data_len = len(raw.times)\n",
    "    nepochs = int(np.floor(data_len / (target_fs * 30)) - 1)\n",
    "\n",
    "    # Create events from annotations\n",
    "    events, event_id = mne.events_from_annotations(raw)\n",
    "    event_id_remap = {v: event_id_global_map[k] for k, v in event_id.items()}\n",
    "\n",
    "    # epoch_file =  mne.Epochs(file, events, tmin=0, tmax=5,baseline=None)\n",
    "    epoch_file = mne.Epochs(raw, events, tmin=0., tmax=30.0, baseline=None, preload=True) \n",
    "\n",
    "    dataframe = epoch_file.to_data_frame()\n",
    "\n",
    "    dataframe.condition = dataframe.condition.apply(lambda x: event_id_remap[int(x)])\n",
    "\n",
    "    epochs = dataframe.epoch\n",
    "    condition = dataframe.condition\n",
    "    df = dataframe[channels_to_use]\n",
    "\n",
    "    # Z-Score Each Channel\n",
    "    mean_vals = df.mean(axis=0)\n",
    "    sd_vals = df.std(axis=0)\n",
    "\n",
    "    for ch in channels_to_use:\n",
    "        df[ch] -= mean_vals[ch]\n",
    "        df[ch] /= sd_vals[ch]\n",
    "\n",
    "    \n",
    "\n",
    "    count = 0\n",
    "    for epoch in sorted(np.unique(epochs)):\n",
    "        cur_label = int(condition[epochs == epoch].iloc[0])\n",
    "        single_channel = np.array(df[epochs == epoch]).transpose()[None]\n",
    "        multi_channel = np.repeat(single_channel, repeats=19, axis=1) \n",
    "        multi_channel += np.random.normal(scale=0.7, size=multi_channel.shape)\n",
    "        vals = multi_channel[...,:30*target_fs]\n",
    "        data.append(vals)\n",
    "        label.append(cur_label)\n",
    "        count += 1\n",
    "\n",
    "    subj_real = int(data_files[subj][3:5])\n",
    "    subject.extend(list((subj_real*np.ones((count,))).astype(int)))\n",
    "\n",
    "data_final = np.concatenate(data, axis=0)\n",
    "label_final = np.array(label)\n",
    "subject_final = np.array(subject)\n",
    "\n",
    "filename1 = 'Spectral_Explainability/segmented_sc_data.pkl'\n",
    "save_data1 = {'data':data_final,'subject':subject_final,'label':label_final}\n",
    "\n",
    "with open(filename1, 'wb') as f:\n",
    "    pickle.dump(save_data1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13137, 19, 3000)\n",
      "(13137,)\n",
      "(13137,)\n"
     ]
    }
   ],
   "source": [
    "# Generic ML Libraries\n",
    "import sklearn\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "# General Libraries\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat, savemat\n",
    "from scipy.fft import fft, fftfreq, ifft\n",
    "import h5py\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Figure Libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Polygon\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.optim as optim\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "## IMPORT DATA\n",
    "\n",
    "datadir = \"Spectral_Explainability\"\n",
    "filename = \"segmented_sc_data.pkl\"\n",
    "\n",
    "with open(os.path.join(datadir,filename), 'rb') as f:\n",
    "    mat_file = pickle.load(f)\n",
    "X = np.float32(mat_file['data']); # data\n",
    "Y = np.float32(mat_file['label']); # labels\n",
    "S = np.float32(mat_file['subject']); # subject number\n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(Y))\n",
    "print(np.shape(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1.0: 0, 2.0: 1, 3.0: 2, 6.0: 3, 7.0: 4}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_targets = {x: i for i, x in enumerate(np.unique(Y))}\n",
    "map_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = np.array([map_targets[x] for x in Y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelMDD(\n",
      "  (channel_dropout): ChannelDropout(rate=0.25, noise_shape=None, seed=None)\n",
      "  (conv1): Conv1d(19, 5, kernel_size=(10,), stride=(1,))\n",
      "  (conv2): Conv1d(5, 10, kernel_size=(10,), stride=(1,))\n",
      "  (conv3): Conv1d(10, 10, kernel_size=(10,), stride=(1,))\n",
      "  (conv4): Conv1d(10, 15, kernel_size=(5,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn3): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (bn4): BatchNorm1d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc_bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=2715, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=5, bias=True)\n",
      "  (dropout): AlphaDropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "## ChannelDropout Code\n",
    "class ChannelDropout(nn.Module):\n",
    "    def __init__(self, rate, noise_shape=None, seed=None):\n",
    "        super().__init__()\n",
    "        self.rate = min(1., max(0., rate))\n",
    "        self.noise_shape = noise_shape\n",
    "        self.seed = seed\n",
    "        if seed is not None:\n",
    "            torch.manual_seed(seed)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        if self.training:\n",
    "            noise_shape = [1, 1, inputs.shape[2]]\n",
    "            mask = torch.bernoulli(torch.full(noise_shape, 1 - self.rate)).to(inputs.device)\n",
    "            mask = mask.expand_as(inputs)\n",
    "            return inputs * mask / (1 - self.rate)\n",
    "        else:\n",
    "            return inputs\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return f'rate={self.rate}, noise_shape={self.noise_shape}, seed={self.seed}'\n",
    "    \n",
    "\n",
    "class ModelMDD(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.n_features = 19\n",
    "        self.n_points = 3000\n",
    "\n",
    "        self.channel_dropout = ChannelDropout(rate=0.25)\n",
    "        \n",
    "        # Conv1D layers\n",
    "        self.conv1 = nn.Conv1d(self.n_features, 5, kernel_size=10, stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv1d(5, 10, kernel_size=10, stride=1, padding=0)\n",
    "        self.conv3 = nn.Conv1d(10, 10, kernel_size=10, stride=1, padding=0)\n",
    "        self.conv4 = nn.Conv1d(10, 15, kernel_size=5, stride=1, padding=0)\n",
    "        \n",
    "        # BatchNorm layers\n",
    "        self.bn1 = nn.BatchNorm1d(5)\n",
    "        self.bn2 = nn.BatchNorm1d(10)\n",
    "        self.bn3 = nn.BatchNorm1d(10)\n",
    "        self.bn4 = nn.BatchNorm1d(15)\n",
    "\n",
    "        # BatchNorm layers\n",
    "        self.fc_bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc_bn2 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # Dense layers\n",
    "        self.fc1 = nn.Linear(15 * 181, 256)  # 184 is calculated based on the input size and conv/pool operations\n",
    "        self.fc2 = nn.Linear(256, 64)\n",
    "        self.fc3 = nn.Linear(64, 5)\n",
    "        \n",
    "        self.dropout = nn.AlphaDropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # channel dropout\n",
    "        x = self.channel_dropout(x)\n",
    "\n",
    "        # Conv layers\n",
    "        x = F.elu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        x = F.elu(self.conv4(x))\n",
    "        x = F.max_pool1d(x, 2)\n",
    "        x = self.bn4(x)\n",
    "        \n",
    "        # Flatten\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # Dense layers\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.fc_bn1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = F.elu(self.fc_bn2(self.fc2(x)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x) # F.softmax(, dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "def get_model(dropout=0.5):\n",
    "    model = ModelMDD(dropout)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.train()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "def evaluate_model(X_train, X_val, Y_train, Y_val, checkpoint_path):\n",
    "    # Convert numpy arrays to PyTorch tensors\n",
    "    X_train = torch.FloatTensor(X_train)\n",
    "    X_val = torch.FloatTensor(X_val)\n",
    "    Y_train = torch.LongTensor(Y_train)\n",
    "    Y_val = torch.LongTensor(Y_val)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_dataset = TensorDataset(X_train, Y_train)\n",
    "    val_dataset = TensorDataset(X_val, Y_val)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=128)\n",
    "\n",
    "    # Get model\n",
    "    model = get_model()\n",
    "    model.to(device)\n",
    "\n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(Y_train.numpy()), \n",
    "                                         y=Y_train.numpy().squeeze())\n",
    "    class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=5, verbose=True)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 100\n",
    "    best_val_acc = 0\n",
    "    patience = 20\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        train_acc = train_correct / train_total\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                \n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        val_acc = val_correct / val_total\n",
    "\n",
    "        # Print epoch results\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        # Learning rate scheduler step\n",
    "        scheduler.step(val_acc)\n",
    "\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "\n",
    "        # # Early stopping\n",
    "        # if val_acc > best_val_acc:\n",
    "        #     best_val_acc = val_acc\n",
    "        #     counter = 0\n",
    "        #     torch.save(model.state_dict(), checkpoint_path)\n",
    "        # else:\n",
    "        #     counter += 1\n",
    "        #     if counter >= patience:\n",
    "        #         print(\"Early stopping\")\n",
    "        #         break\n",
    "\n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load(checkpoint_path))\n",
    "\n",
    "    return model  # Return None instead of history, as PyTorch doesn't have a built-in history object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = 'trim_begin_end_1000_wo_wake_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdefeeb3a4941b08720fa2df52c0f53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Split:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansafronov/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (19) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msleep_pretrain_ckpt/sleep_model_Fold\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;28mstr\u001b[39m(count)\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m X_test \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_test)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(model(X_test), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "Cell \u001b[0;32mIn[23], line 47\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(X_train, X_val, Y_train, Y_val, checkpoint_path)\u001b[0m\n\u001b[1;32m     44\u001b[0m inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     45\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     50\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[22], line 66\u001b[0m, in \u001b[0;36mModelMDD.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))\n\u001b[1;32m     65\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmax_pool1d(x, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 66\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresblock_lin_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_res\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m x_res \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39melu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (19) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# Run Classifier for 10 Folds\n",
    "n_folds = 10\n",
    "Y_pred = []; Y_test_all = []; Y_pred_val = []; Y_val_all = [];\n",
    "val_loss = []; train_loss = [];\n",
    "val_acc = []; train_acc = [];\n",
    "confusion_matrices = [];\n",
    "Sample_Idx = np.expand_dims(np.arange(np.shape(Y)[0]),axis=1) \n",
    "\n",
    "count = 0\n",
    "# split data into Train/Val and Test Groups\n",
    "cv = GroupShuffleSplit(n_splits=n_folds,test_size=0.1,train_size=0.9,random_state=0)\n",
    "for train_val_idx, test_idx in tqdm(cv.split(X,Y,S), desc=f\"Split\", total=n_folds):\n",
    "    X_train_val = X[train_val_idx,...]\n",
    "    Y_train_val = Y[train_val_idx,...]\n",
    "    S_train_val = S[train_val_idx,...]\n",
    "    X_test = X[test_idx,...]\n",
    "    Y_test = Y[test_idx,...]\n",
    "    S_test = S[test_idx,...]\n",
    "    Sample_Idx_Test = Sample_Idx[test_idx,...]\n",
    "    \n",
    "    # Split Train/Val Data into Training and Validation Groups\n",
    "    cv2 = GroupShuffleSplit(n_splits=1,test_size=0.10,train_size=0.90,random_state=0)\n",
    "    for train_idx, val_idx in cv2.split(X_train_val,Y_train_val,S_train_val):\n",
    "        X_train = X_train_val[train_idx,...]\n",
    "        Y_train = Y_train_val[train_idx,...]\n",
    "        S_train = S_train_val[train_idx,...]\n",
    "        X_val = X_train_val[val_idx,...]\n",
    "        Y_val = Y_train_val[val_idx,...]\n",
    "        S_val = S_train_val[val_idx,...]\n",
    "    X_train_val = []; Y_train_val = []; S_train_val = []\n",
    "    \n",
    "    # Define Model Checkpoints\n",
    "    if prefix:\n",
    "        directory = f\"sleep_pretrain_ckpt/{prefix}\"\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "        file_path = f\"{directory}/sleep_model_Fold\"+str(count)+\".pt\"\n",
    "    else:\n",
    "        file_path = \"sleep_pretrain_ckpt/sleep_model_Fold\"+str(count)+\".pt\"\n",
    "\n",
    "    # Evaluate model\n",
    "    model = evaluate_model(X_train, X_val, Y_train, Y_val, checkpoint_path=file_path)\n",
    "\n",
    "    X_test = torch.tensor(X_test).to(device)\n",
    "    y_pred = torch.argmax(model(X_test), dim=1).cpu().data.numpy()\n",
    "\n",
    "    confusion_matrices.append(confusion_matrix(Y_test, y_pred))\n",
    "    \n",
    "    print(count)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Awake': 4, 'NREM1': 0, 'NREM2': 1, 'NREM3': 2, 'REM': 3}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_metrics(confusion_matrices, class_mapping):\n",
    "    n_classes = len(class_mapping)\n",
    "    \n",
    "    # Initialize arrays to store metrics\n",
    "    precision = np.zeros((len(confusion_matrices), n_classes))\n",
    "    recall = np.zeros((len(confusion_matrices), n_classes))\n",
    "    f1_score = np.zeros((len(confusion_matrices), n_classes))\n",
    "    \n",
    "    for i, cm in enumerate(confusion_matrices):\n",
    "        for class_name, class_index in class_mapping.items():\n",
    "            tp = cm[class_index, class_index]\n",
    "            fp = np.sum(cm[:, class_index]) - tp\n",
    "            fn = np.sum(cm[class_index, :]) - tp\n",
    "            \n",
    "            # Calculate precision\n",
    "            precision[i, class_index] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            \n",
    "            # Calculate recall\n",
    "            recall[i, class_index] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            \n",
    "            # Calculate F1-score\n",
    "            f1_score[i, class_index] = 2 * (precision[i, class_index] * recall[i, class_index]) / (precision[i, class_index] + recall[i, class_index]) if (precision[i, class_index] + recall[i, class_index]) > 0 else 0\n",
    "    \n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_precision = np.mean(precision, axis=0) * 100\n",
    "    std_precision = np.std(precision, axis=0) * 100\n",
    "    mean_recall = np.mean(recall, axis=0) * 100\n",
    "    std_recall = np.std(recall, axis=0) * 100\n",
    "    mean_f1 = np.mean(f1_score, axis=0) * 100\n",
    "    std_f1 = np.std(f1_score, axis=0) * 100\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Class\\t\\tPrecision\\t\\tRecall\\t\\t\\tF1-Score\")\n",
    "    print(\"-\" * 70)\n",
    "    for class_name, class_index in class_mapping.items():\n",
    "        print(f\"{class_name}\\t\\t{mean_precision[class_index]:.2f}±{std_precision[class_index]:.2f}\\t\\t{mean_recall[class_index]:.2f}±{std_recall[class_index]:.2f}\\t\\t{mean_f1[class_index]:.2f}±{std_f1[class_index]:.2f}\")\n",
    "\n",
    "    \n",
    "class_mapping = {\n",
    "    \"Awake\": 4,\n",
    "    \"NREM1\": 0,\n",
    "    \"NREM2\": 1,\n",
    "    \"NREM3\": 2,\n",
    "    \"REM\": 3\n",
    "}\n",
    "\n",
    "class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrices' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m calculate_metrics(\u001b[43mconfusion_matrices\u001b[49m, class_mapping)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'confusion_matrices' is not defined"
     ]
    }
   ],
   "source": [
    "calculate_metrics(confusion_matrices, class_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 06:53:44.433558: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-10-25 06:53:44.461860: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-25 06:53:45.079096: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Layer\n",
    "import keras\n",
    "\n",
    "class ChannelDropout(Layer):\n",
    "    def __init__(self, rate, noise_shape=None, seed=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.rate = min(1., max(0., rate))\n",
    "        self.noise_shape = noise_shape\n",
    "        self.seed = seed\n",
    "    \n",
    "    def call(self, inputs, training = None):\n",
    "        return tf.nn.dropout(inputs, \n",
    "                             rate=self.rate, \n",
    "                            noise_shape = [1,1,inputs.shape[2]])\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {\n",
    "            \"rate\": self.rate,\n",
    "            \"noise_shape\": self.noise_shape,\n",
    "            \"seed\": self.seed,\n",
    "        }\n",
    "        base_config = super().get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 06:53:45.763176: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:995] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-10-25 06:53:45.764087: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "custom_objects = {'ChannelDropout': ChannelDropout}\n",
    "with keras.saving.custom_object_scope(custom_objects):\n",
    "    keras_model = keras.models.load_model('Pretraining/Sleep_Models/sleep_model_Fold9.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 15/411 [>.............................] - ETA: 3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 06:55:02.308801: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 2995236000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "411/411 [==============================] - 3s 7ms/step\n"
     ]
    }
   ],
   "source": [
    "out = keras_model.predict(X.transpose(0, 2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 1, 2, 2, 2, 2, 1, 2, 1,\n",
       "       2, 0, 1, 2, 1, 2, 1, 2, 2, 2, 1, 2, 2, 2, 2, 0, 2, 1, 2, 1, 2, 3,\n",
       "       0, 1, 2, 0, 1, 0, 1, 2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 0, 1, 0, 1, 0, 1, 0, 1, 3, 1, 0, 4, 0, 1, 0,\n",
       "       1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 2, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 3, 3, 3, 3, 3, 1, 2, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 2,\n",
       "       2, 2, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2,\n",
       "       2, 2, 4, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 2, 2, 2, 2, 2, 3,\n",
       "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(out, axis=1)[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4411E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4541F0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4341F0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4061E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4221E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4171E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4501E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4641E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4122E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4011E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4022E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4051E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4372F0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4101E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4291G0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4001E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4532E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4471F0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4701E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4731E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4772G0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4241E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4671G0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4522E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Progress: 60 / 137\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4811G0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4331F0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4302E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4232E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4602E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4112E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4152E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4292G0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "\n",
      "Progress: 110 / 137\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4632E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4121E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Progress: 120 / 137\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4592G0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "Progress: 130 / 137\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4622E0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different highpass filters. Highest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Channels contain different lowpass filters. Lowest filter setting will be stored.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
      "/tmp/ipykernel_688892/1482406991.py:39: RuntimeWarning: Highpass cutoff frequency 16.0 is greater than lowpass cutoff frequency 0.7, setting values to 0 and Nyquist.\n",
      "  data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating raw.info structure...\n",
      "\n",
      "start test process!\n",
      "Extracting EDF parameters from /home/ansafronov/Yandex.Disk/Studies/neuroml/project/physionet.org/files/sleep-edfx/1.0.0/sleep-cassette/SC4382F0-PSG.edf...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import mne\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from multiprocessing import Process\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "def pretext_train_test(root_folder, k, N, epoch_sec):\n",
    "    all_index = np.unique([path[:6] for path in os.listdir(root_folder)])\n",
    "    \n",
    "    pretext_index = np.random.choice(all_index, int(len(all_index) * 0.9), replace=False)\n",
    "    train_index = np.random.choice(list(set(all_index) - set(pretext_index)), int(len(all_index) * 0.05), replace=False)\n",
    "    test_index = list(set(all_index) - set(pretext_index) - set(train_index))\n",
    "\n",
    "    print ('start pretext process!')\n",
    "    sample_process(root_folder, k, N, epoch_sec, 'pretext', pretext_index)\n",
    "    print ()\n",
    "    \n",
    "    print ('start train process!')\n",
    "    sample_process(root_folder, k, N, epoch_sec, 'train', train_index)\n",
    "    print ()\n",
    "    \n",
    "    print ('start test process!')    \n",
    "    sample_process(root_folder, k, N, epoch_sec, 'test', test_index)\n",
    "    print ()\n",
    "\n",
    "\n",
    "def sample_process(root_folder, k, N, epoch_sec, train_test_val, index):\n",
    "    for i, j in enumerate(index):\n",
    "        if i % N == k:\n",
    "            if k == 0:\n",
    "                print ('Progress: {} / {}'.format(i, len(index)))\n",
    "\n",
    "            # load signal \"X\" part\n",
    "            data = mne.io.read_raw_edf(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('PSG' in x), os.listdir(root_folder)))[0])\n",
    "            X = data.get_data()[:2, :]\n",
    "            \n",
    "            # load label \"Y\" part\n",
    "            ann = mne.read_annotations(root_folder + '/' + list(filter(lambda x: (x[:6] == j) and ('Hypnogram' in x), os.listdir(root_folder)))[0])\n",
    "\n",
    "            offset = 5\n",
    "            delete_annotations = [i for i, x in enumerate(ann) if x['description']=='Sleep stage W' and (i < offset or i >= len(ann) - offset)]\n",
    "            ann.delete(delete_annotations)\n",
    "            delete_annotations = [i for i, x in enumerate(ann) if x['description'] in {'Sleep stage ?', 'Movement time'}]\n",
    "            ann.delete(delete_annotations)\n",
    "\n",
    "            labels = []\n",
    "            for dur, des in zip(ann.duration, ann.description):\n",
    "                for i in range(int(dur) // 30):\n",
    "                    labels.append(des[-1])\n",
    "\n",
    "            # slice the EEG signals into non-overlapping windows, window size = sampling rate per second * second time = 100 * windowsize\n",
    "            for slice_index in range(X.shape[1] // (100 * epoch_sec)):\n",
    "                if slice_index >= len(labels):\n",
    "                    break\n",
    "                \n",
    "                # ingore the no labels\n",
    "                path = './Spectral_Explainability/cassette_processed/{}/'.format(train_test_val) + 'cassette-' + j + '-' + str(slice_index) + '.pkl'\n",
    "                pickle.dump({'X': X[:, slice_index * 100 * epoch_sec: (slice_index+1) * 100 * epoch_sec], \\\n",
    "                    'y': labels[slice_index]}, open(path, 'wb'))\n",
    "\n",
    "    \n",
    "if not os.path.exists('./Spectral_Explainability/cassette_processed'):\n",
    "    os.makedirs('./Spectral_Explainability/cassette_processed/pretext')\n",
    "    os.makedirs('./Spectral_Explainability/cassette_processed/train')\n",
    "    os.makedirs('./Spectral_Explainability/cassette_processed/test')\n",
    "\n",
    "root_folder = './physionet.org/files/sleep-edfx/1.0.0/sleep-cassette'\n",
    "\n",
    "N, epoch_sec = 10, 30\n",
    "p_list = []\n",
    "for k in range(N):\n",
    "    process = Process(target=pretext_train_test, args=(root_folder, k, N, epoch_sec))\n",
    "    process.start()\n",
    "    p_list.append(process)\n",
    "\n",
    "for i in p_list:\n",
    "    i.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "import random\n",
    "\n",
    "# Residual Block\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=False, pooling=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ELU(inplace=True)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.maxpool = nn.MaxPool1d(2, stride=2) \n",
    "        self.downsample = nn.Sequential(\n",
    "           nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1),\n",
    "           nn.BatchNorm1d(out_channels)\n",
    "        )\n",
    "        self.downsampleOrNot = downsample\n",
    "        self.pooling = pooling\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        # out = self.dropout(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsampleOrNot:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        if self.pooling:\n",
    "            out = self.maxpool(out)\n",
    "        out = self.dropout(out)\n",
    "        return out\n",
    "\n",
    "class CNNEncoder1d_SLEEP(nn.Module):\n",
    "    def __init__(self, n_dim):\n",
    "        super(CNNEncoder1d_SLEEP, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(4, 6, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm1d(6),\n",
    "            nn.ELU(inplace=True),\n",
    "        )\n",
    "        self.conv2 = ResBlock(6, 8, 2, True, False)\n",
    "        self.conv3 = ResBlock(8, 16, 2, True, True)\n",
    "        self.conv4 = ResBlock(16, 32, 2, True, True)\n",
    "        self.n_dim = n_dim\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, self.n_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_dim, self.n_dim, bias=True),\n",
    "        )\n",
    "\n",
    "        self.sup = nn.Sequential(\n",
    "            nn.Linear(128, 32, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 5, bias=True),\n",
    "        )\n",
    "\n",
    "        self.byol_mapping = nn.Sequential(\n",
    "            nn.Linear(128, self.n_dim, bias=True),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.n_dim, self.n_dim, bias=True),\n",
    "        )\n",
    "\n",
    "    def torch_stft(self, X_train):\n",
    "        signal = []\n",
    "\n",
    "        for s in range(X_train.shape[1]):\n",
    "            spectral = torch.stft(X_train[:, s, :],\n",
    "                n_fft = 256,\n",
    "                hop_length = 256 * 1 // 4,\n",
    "                center = False,\n",
    "                onesided = True,\n",
    "                return_complex=False)\n",
    "            signal.append(spectral)\n",
    "        \n",
    "        signal1 = torch.stack(signal)[:, :, :, :, 0].permute(1, 0, 2, 3)\n",
    "        signal2 = torch.stack(signal)[:, :, :, :, 1].permute(1, 0, 2, 3)\n",
    "\n",
    "        return torch.cat([torch.log(torch.abs(signal1) + 1e-8), torch.log(torch.abs(signal2) + 1e-8)], dim=1)\n",
    "\n",
    "    def forward(self, x, simsiam=False, mid=True, byol=False, sup=False):\n",
    "        x = self.torch_stft(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        if sup:\n",
    "            return self.sup(x)\n",
    "        elif simsiam:\n",
    "            return x, self.fc(x)\n",
    "        elif mid:\n",
    "            return x\n",
    "        elif byol:\n",
    "            x = self.fc(x)\n",
    "            x = self.byol_mapping(x)\n",
    "            return x\n",
    "        else:\n",
    "            x = self.fc(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretext (all patient):  414259\n",
      "train (all patient):  33195\n",
      "test (all) patient):  83077\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelMDD(\n",
       "  (channel_dropout): ChannelDropout(rate=0.25, noise_shape=None, seed=None)\n",
       "  (conv1): Conv1d(19, 5, kernel_size=(10,), stride=(1,))\n",
       "  (conv2): Conv1d(5, 10, kernel_size=(10,), stride=(1,))\n",
       "  (conv3): Conv1d(10, 10, kernel_size=(10,), stride=(1,))\n",
       "  (conv4): Conv1d(10, 15, kernel_size=(5,), stride=(1,))\n",
       "  (bn1): BatchNorm1d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn3): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn4): BatchNorm1d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc_bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc_bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=2715, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
       "  (fc3): Linear(in_features=64, out_features=5, bias=True)\n",
       "  (dropout): AlphaDropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "class SLEEPCALoader(torch.utils.data.Dataset):\n",
    "    def __init__(self, list_IDs, dir, SS=True):\n",
    "        self.list_IDs = list_IDs\n",
    "        self.dir = dir\n",
    "        self.SS = SS\n",
    "\n",
    "        self.label_list = ['W', 'R', 1, 2, 3]\n",
    "        self.bandpass1 = (1, 5)\n",
    "        self.bandpass2 = (30, 49)\n",
    "        self.n_length = 100 * 30\n",
    "        self.n_channels = 19\n",
    "        self.n_classes = 5\n",
    "        self.signal_freq = 100\n",
    "        self.bound = 0.00025\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def corupt(self, x):\n",
    "        \"\"\"\n",
    "        Add noise to multiple ts\n",
    "        Input: \n",
    "            x: (n_channel, n_length)\n",
    "        Output: \n",
    "            x: (n_channel, n_length)\n",
    "        \"\"\"\n",
    "        single_channel = x[0,:]\n",
    "        single_channel = (single_channel - single_channel.mean()) / single_channel.std() \n",
    "        single_channel = single_channel[None, :]\n",
    "\n",
    "        multi_channel = np.repeat(single_channel, repeats=19, axis=0) \n",
    "        multi_channel += np.random.normal(scale=0.7, size=multi_channel.shape)\n",
    "        return multi_channel\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        path = self.dir + self.list_IDs[index]\n",
    "        sample = pickle.load(open(path, 'rb'))\n",
    "        X, y = sample['X'], sample['y']\n",
    "        \n",
    "        # original y.unique = [0, 1, 2, 3, 5]\n",
    "        if y == 'W':\n",
    "            y = 0\n",
    "        elif y == 'R':\n",
    "            y = 4\n",
    "        elif y in ['1', '2', '3']:\n",
    "            y = int(y)\n",
    "        elif y == '4':\n",
    "            y = 3\n",
    "        else:\n",
    "            y = 0\n",
    "        \n",
    "        y = torch.LongTensor([y])\n",
    "\n",
    "        X = self.corupt(X)\n",
    "\n",
    "        return torch.FloatTensor(X), y\n",
    "\n",
    "pretext_dir = './Spectral_Explainability/cassette_processed/pretext/'\n",
    "train_dir = './Spectral_Explainability/cassette_processed/train/'\n",
    "test_dir = './Spectral_Explainability/cassette_processed/test/'\n",
    "\n",
    "pretext_index = os.listdir(pretext_dir)\n",
    "train_index = os.listdir(train_dir)\n",
    "train_index = train_index[:len(train_index)//2]\n",
    "test_index = os.listdir(test_dir)\n",
    "\n",
    "print ('pretext (all patient): ', len(pretext_index))\n",
    "print ('train (all patient): ', len(train_index))\n",
    "print ('test (all) patient): ', len(test_index))\n",
    "\n",
    "pretext_loader = torch.utils.data.DataLoader(SLEEPCALoader(pretext_index, pretext_dir, True), \n",
    "                batch_size=128, shuffle=True)\n",
    "train_loader = torch.utils.data.DataLoader(SLEEPCALoader(train_index, train_dir, False), \n",
    "                batch_size=128, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(SLEEPCALoader(test_index, test_dir, False), \n",
    "                batch_size=128, shuffle=False)\n",
    "\n",
    "# define and initialize the model\n",
    "device = 'cuda'\n",
    "\n",
    "model = get_model()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.18.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sentry-sdk>=2.0.0 in /home/ansafronov/.local/lib/python3.10/site-packages (from wandb) (2.15.0)\n",
      "Collecting gitpython!=3.1.29,>=1.0.0\n",
      "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 KB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setproctitle in /usr/lib/python3/dist-packages (from wandb) (1.2.2)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from wandb) (5.4.1)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Using cached docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Requirement already satisfied: platformdirs in /home/ansafronov/.local/lib/python3.10/site-packages (from wandb) (4.2.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/lib/python3/dist-packages (from wandb) (8.0.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from wandb) (59.6.0)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4 in /home/ansafronov/.local/lib/python3.10/site-packages (from wandb) (4.5.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/ansafronov/.local/lib/python3.10/site-packages (from wandb) (4.25.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ansafronov/.local/lib/python3.10/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /home/ansafronov/.local/lib/python3.10/site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/lib/python3/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ansafronov/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ansafronov/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ansafronov/.local/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: smmap, docker-pycreds, gitdb, gitpython, wandb\n",
      "Successfully installed docker-pycreds-0.4.0 gitdb-4.0.11 gitpython-3.1.43 smmap-5.0.1 wandb-0.18.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/ansafronov/neroml/734cdb67211149178a5308e85dfca84c\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "  api_key=\"sPRzejvIjrBCMFoZZDmZSAec3\",\n",
    "  project_name=\"neroml\",\n",
    "  workspace=\"ansafronov\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import gc \n",
    "from comet_ml import Experiment\n",
    "\n",
    "def train(model, optimizer, Epoch, loss_func, train_loader, test_loader):\n",
    "\n",
    "    model.train()\n",
    "    acc_list = []\n",
    "\n",
    "    experiment.log_parameters({\n",
    "        \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "        \"batch_size\": train_loader.batch_size,\n",
    "        \"epochs\": Epoch,\n",
    "        \"optimizer\": \"Adam\",\n",
    "        \"model_architecture\": \"CNN\"\n",
    "    })\n",
    "\n",
    "    # train\n",
    "    for epoch in range(Epoch):\n",
    "        print ()\n",
    "        # Train\n",
    "        correct_train, total_train, loss_train = [], [], []\n",
    "        max_min = [0,0]\n",
    "        conf_matrix = np.zeros((5, 5))\n",
    "        for idx, (X_train, y_train) in enumerate(tqdm(train_loader, desc='Training')):\n",
    "            X_train, y_train = X_train, y_train.flatten()\n",
    "            X_train, y_train = X_train.to(device), y_train.to(device)\n",
    "\n",
    "            # backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            pred = model(X_train)\n",
    "            loss = loss_func(pred, y_train)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # print ('compute accuracy')\n",
    "            total_train.append(y_train.shape[0])\n",
    "            correct_train.append((torch.argmax(pred.data, 1) == y_train).sum().item())\n",
    "            loss_train.append(loss.item())\n",
    "\n",
    "            # conf_matrix += confusion_matrix(y_train.detach().cpu().numpy(), torch.argmax(pred.data, 1).detach().cpu().numpy())\n",
    "\n",
    "        print(conf_matrix)\n",
    "            \n",
    "        print (\"epoch: {}, avg_loss: {}, train accuracy: {:.2f}%\".format(epoch, sum(loss_train) / len(loss_train), sum(correct_train) / sum(total_train) * 100))\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "        # # evaluation\n",
    "        # with torch.no_grad():\n",
    "        #     model.eval()\n",
    "            \n",
    "\n",
    "        #     conf_matrix = np.zeros((5, 5))\n",
    "        #     correct_test, total_test = 0, 0\n",
    "        #     for idx, (X_test, y_test) in enumerate(tqdm(test_loader, desc='Evaluating')):\n",
    "        #         X_test, y_test = X_test, y_test.flatten()\n",
    "        #         X_test, y_test = X_test.to(device), y_test\n",
    "\n",
    "        #         y_pred = torch.argmax(model(X_test), 1).detach().cpu().numpy()\n",
    "        #         correct_test += np.sum(y_pred == y_test.numpy())\n",
    "        #         total_test += y_test.numpy().shape[0]\n",
    "\n",
    "        #         conf_matrix += confusion_matrix(y_test.numpy(), y_pred)\n",
    "\n",
    "        #     print (\"------------------------------\")\n",
    "        #     print (\"epoch: {}, train accuracy: {:.2f}%, test accuracy: {:.2f}%\".format(epoch, sum(correct_train) / sum(total_train) * 100, correct_test / total_test * 100))\n",
    "\n",
    "        #     print(conf_matrix)\n",
    "            \n",
    "        #     acc_list.append(correct_test / total_test)\n",
    "        #     # print (confusion_matrix(pred, target))\n",
    "\n",
    "        #     model.train()\n",
    "        \n",
    "        if epoch > 10:\n",
    "            print ('recent five epoch, mean: {}, std: {}'.format(np.mean(acc_list[-10:]), np.std(acc_list[-10:])))\n",
    "\n",
    "        experiment.log_metric(\"loss\", loss, step=epoch)\n",
    "        experiment.log_metric(\"accuracy\", sum(correct_train) / sum(total_train) * 100, step=epoch)\n",
    "\n",
    "        torch.save(model.state_dict(), 'sleep_pretrain.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m loss_func \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m train(model, optimizer, \u001b[38;5;241m20\u001b[39m, loss_func, \u001b[43mtrain_loader\u001b[49m, test_loader)\n\u001b[1;32m     10\u001b[0m experiment\u001b[38;5;241m.\u001b[39mend()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.078)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# training\n",
    "train(model, optimizer, 20, loss_func, train_loader, test_loader)\n",
    "\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dotmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01myaml\u001b[39;00m\n",
      "\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotmap\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DotMap\n",
      "\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_config_from_yaml\u001b[39m(yaml_file):\n",
      "\u001b[1;32m      9\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "\u001b[1;32m     10\u001b[0m \u001b[38;5;124;03m    Get the config from a yaml file\u001b[39;00m\n",
      "\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03m    :param yaml_file:\u001b[39;00m\n",
      "\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    :return: config(namespace) or config(dictionary)\u001b[39;00m\n",
      "\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dotmap'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import yaml\n",
    "from dotmap import DotMap\n",
    "\n",
    "\n",
    "def get_config_from_yaml(yaml_file):\n",
    "    \"\"\"\n",
    "    Get the config from a yaml file\n",
    "    :param yaml_file:\n",
    "    :return: config(namespace) or config(dictionary)\n",
    "    \"\"\"\n",
    "    # parse the configurations from the config yaml file provided\n",
    "    with open(yaml_file, 'r') as config_file:\n",
    "        config_dict = yaml.safe_load(config_file)\n",
    "\n",
    "    # convert the dictionary to a namespace using bunch lib\n",
    "    config = DotMap(config_dict)\n",
    "\n",
    "    return config, config_dict\n",
    "\n",
    "\n",
    "def process_config(yaml_file):\n",
    "    config, _ = get_config_from_yaml(yaml_file)\n",
    "\n",
    "    return config\n",
    "\n",
    "config = process_config('config.yaml')\n",
    "\n",
    "class RnnModel(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "        # Assign parameters\n",
    "        self.filter_base = config.network.filter_base\n",
    "        self.kernel_size = config.network.kernel_size\n",
    "        self.max_pooling = config.network.max_pooling\n",
    "        self.num_blocks = config.network.num_blocks\n",
    "        self.num_channels = len(config.data_loader.modalities) + 1\n",
    "        self.num_classes = config.data_loader.num_classes\n",
    "        self.rnn_bidirectional = config.network.rnn_bidirectional\n",
    "        self.rnn_num_layers = config.network.rnn_num_layers\n",
    "        self.rnn_num_units = config.network.rnn_num_units if config.network.rnn_num_units is not None else 4 * \\\n",
    "            self.filter_base * (2 ** (self.num_blocks - 1))\n",
    "\n",
    "        # Create network\n",
    "        if self.num_channels != 1:\n",
    "            self.mixing_block = nn.Sequential(OrderedDict([\n",
    "                ('mix_conv', nn.Conv2d(1, self.num_channels, (self.num_channels, 1))),\n",
    "                ('mix_batchnorm', nn.BatchNorm2d(self.num_channels)),\n",
    "                ('mix_relu', nn.ReLU())\n",
    "            ]))\n",
    "\n",
    "        # Define shortcut\n",
    "        self.shortcuts = nn.ModuleList([\n",
    "            nn.Sequential(OrderedDict([\n",
    "                ('shortcut_conv_{}'.format(k), nn.Conv2d(\n",
    "                    in_channels=self.num_channels if k == 0 else 4 *\n",
    "                    self.filter_base * (2 ** (k - 1)),\n",
    "                    out_channels=4 * self.filter_base * (2 ** k),\n",
    "                    kernel_size=(1, 1)))\n",
    "            ])) for k in range(self.num_blocks)\n",
    "        ])\n",
    "\n",
    "        # Define basic block structure\n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.Sequential(OrderedDict([\n",
    "                (\"conv_{}_1\".format(k), nn.Conv2d(\n",
    "                    in_channels=self.num_channels if k == 0 else 4 * self.filter_base *\n",
    "                    (2 ** (k - 1)),\n",
    "                    out_channels=self.filter_base * (2 ** k),\n",
    "                    kernel_size=(1, 1))),\n",
    "                # (\"padding_{}\".format(k), nn.ConstantPad2d([1, 1, 0, 0], 0)),\n",
    "                (\"batchnorm_{}_1\".format(k), nn.BatchNorm2d(\n",
    "                    self.filter_base * (2 ** k))),\n",
    "                (\"relu_{}_1\".format(k), nn.ReLU()),\n",
    "                (\"conv_{}_2\".format(k), nn.Conv2d(\n",
    "                    in_channels=self.filter_base * (2 ** k),\n",
    "                    out_channels=self.filter_base * (2 ** k),\n",
    "                    kernel_size=(1, self.kernel_size),\n",
    "                    padding=(0, self.kernel_size // 2))),\n",
    "                (\"batchnorm_{}_2\".format(k), nn.BatchNorm2d(\n",
    "                    self.filter_base * (2 ** k))),\n",
    "                (\"relu_{}_2\".format(k), nn.ReLU()),\n",
    "                (\"conv_{}_3\".format(k), nn.Conv2d(\n",
    "                    in_channels=self.filter_base * (2 ** k),\n",
    "                    out_channels=4 * self.filter_base * (2 ** k),\n",
    "                    kernel_size=(1, 1))),\n",
    "                (\"batchnorm_{}_3\".format(k), nn.BatchNorm2d(\n",
    "                    4 * self.filter_base * (2 ** k)))\n",
    "            ])) for k in range(self.num_blocks)\n",
    "        ])\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=(1, self.max_pooling))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        if self.rnn_num_units == 0:\n",
    "\n",
    "            # Classification (outputs only logits)\n",
    "            self.classification = nn.Conv1d(\n",
    "                in_channels=4 * self.filter_base * (2 ** (self.num_blocks - 1)),\n",
    "                out_channels=self.num_classes,\n",
    "                kernel_size=1)\n",
    "        else:\n",
    "\n",
    "            # Temporal processing\n",
    "            self.temporal_block = nn.GRU(\n",
    "                input_size=4 * self.filter_base * (2 ** (self.num_blocks - 1)),\n",
    "                hidden_size=self.rnn_num_units, num_layers=self.rnn_num_layers,\n",
    "                batch_first=True, dropout=0, bidirectional=self.rnn_bidirectional)\n",
    "            self.temporal_block.flatten_parameters()\n",
    "\n",
    "            # Classification (outputs only logits)\n",
    "            self.classification = nn.Conv1d(\n",
    "                in_channels=(1 + self.rnn_bidirectional) * self.rnn_num_units,\n",
    "                out_channels=self.num_classes,\n",
    "                kernel_size=1)\n",
    "        # self.classification = nn.Sequential(OrderedDict([\n",
    "        #     ('cls_conv', nn.Conv1d(\n",
    "        #         in_channels=(1 + self.rnn_bidirectional) *\n",
    "        #         self.rnn_num_units,\n",
    "        #         out_channels=self.num_classes,\n",
    "        #         kernel_size=1)),\n",
    "        #     ('softmax', nn.Softmax(dim=1))\n",
    "        # ]))\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # if self.temporal_block:\n",
    "        #     self.temporal_block.flatten_parameters()\n",
    "\n",
    "        z = self.mixing_block(x)\n",
    "        for block, shortcut in zip(self.blocks, self.shortcuts):\n",
    "            y = shortcut(z)\n",
    "            z = block(z)\n",
    "            z += y\n",
    "            z = self.relu(z)\n",
    "            z = self.maxpool(z)\n",
    "        # print(z.shape)\n",
    "        # RNN part\n",
    "        # print(self.rnn_num_units)\n",
    "        if self.rnn_num_units == 0:\n",
    "            z = self.classification(z.squeeze(2))\n",
    "            # print('Hej! ' + str(z.shape))\n",
    "        else:\n",
    "            z = self.temporal_block(z.squeeze(2).transpose(1, 2))\n",
    "            # print(z[0].shape)\n",
    "            z = self.classification(z[0].transpose(1, 2))\n",
    "\n",
    "        return z\n",
    "    \n",
    "model_1 = RnnModel(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'sleep_pretrain.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  19%|█▊        | 69/369 [00:11<00:49,  6.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m pred, target \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (X_test, y_test) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(test_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvaluating\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m      9\u001b[0m     X_test, y_test \u001b[38;5;241m=\u001b[39m X_test, y_test\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     10\u001b[0m     X_test, y_test \u001b[38;5;241m=\u001b[39m X_test\u001b[38;5;241m.\u001b[39mto(device), y_test\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[88], line 52\u001b[0m, in \u001b[0;36mSLEEPCALoader.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     48\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     50\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([y])\n\u001b[0;32m---> 52\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorupt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mFloatTensor(X), y\n",
      "Cell \u001b[0;32mIn[88], line 30\u001b[0m, in \u001b[0;36mSLEEPCALoader.corupt\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m single_channel \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m0\u001b[39m,:][\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m     29\u001b[0m multi_channel \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(single_channel, repeats\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m19\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m) \n\u001b[0;32m---> 30\u001b[0m multi_channel \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmulti_channel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m multi_channel\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    \n",
    "    pred, target = [], []\n",
    "    for idx, (X_test, y_test) in enumerate(tqdm(test_loader, desc='Evaluating')):\n",
    "        X_test, y_test = X_test, y_test.flatten()\n",
    "        X_test, y_test = X_test.to(device), y_test\n",
    "\n",
    "        y_pred = torch.argmax(model(X_test), 1).data.cpu()\n",
    "        pred += list(y_pred); target += list(y_test.numpy())\n",
    "\n",
    "    conf_matr = confusion_matrix(target, pred)\n",
    "    print(conf_matr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1731, 0.0865, 0.3641, 0.2291, 0.1472],\n",
       "        [0.3443, 0.2227, 0.1733, 0.1562, 0.1035],\n",
       "        [0.1386, 0.1087, 0.4471, 0.1579, 0.1477],\n",
       "        [0.1756, 0.3059, 0.1457, 0.2733, 0.0995],\n",
       "        [0.1338, 0.2841, 0.3023, 0.1501, 0.1296],\n",
       "        [0.2825, 0.1162, 0.1533, 0.1659, 0.2821],\n",
       "        [0.3896, 0.0923, 0.2929, 0.1433, 0.0820],\n",
       "        [0.3027, 0.1222, 0.2159, 0.1681, 0.1911],\n",
       "        [0.0928, 0.1326, 0.2426, 0.4100, 0.1220],\n",
       "        [0.1323, 0.1878, 0.3243, 0.2350, 0.1206],\n",
       "        [0.0697, 0.1921, 0.1856, 0.3282, 0.2244],\n",
       "        [0.2592, 0.1750, 0.3134, 0.2079, 0.0445],\n",
       "        [0.1190, 0.3300, 0.1804, 0.1944, 0.1762],\n",
       "        [0.1325, 0.1038, 0.1483, 0.1965, 0.4190],\n",
       "        [0.1567, 0.1361, 0.1523, 0.3705, 0.1844],\n",
       "        [0.1161, 0.1572, 0.2656, 0.3263, 0.1348],\n",
       "        [0.1515, 0.2233, 0.0951, 0.3053, 0.2248],\n",
       "        [0.3942, 0.1128, 0.3018, 0.1093, 0.0818],\n",
       "        [0.2226, 0.3168, 0.1800, 0.0872, 0.1934],\n",
       "        [0.1957, 0.1268, 0.3267, 0.2393, 0.1114],\n",
       "        [0.1055, 0.3220, 0.1444, 0.2123, 0.2158],\n",
       "        [0.3424, 0.2303, 0.1340, 0.1166, 0.1767],\n",
       "        [0.1843, 0.1345, 0.1042, 0.1538, 0.4232],\n",
       "        [0.1725, 0.1171, 0.2385, 0.2311, 0.2410],\n",
       "        [0.1271, 0.1493, 0.3776, 0.1254, 0.2206],\n",
       "        [0.0850, 0.1367, 0.5099, 0.1418, 0.1266],\n",
       "        [0.1886, 0.2252, 0.1703, 0.2280, 0.1880],\n",
       "        [0.2275, 0.1155, 0.2749, 0.2478, 0.1344],\n",
       "        [0.2156, 0.0959, 0.1688, 0.1989, 0.3208],\n",
       "        [0.1235, 0.3135, 0.0995, 0.0460, 0.4174],\n",
       "        [0.3122, 0.2115, 0.2255, 0.1190, 0.1318],\n",
       "        [0.1820, 0.1228, 0.1762, 0.3884, 0.1306],\n",
       "        [0.3474, 0.1485, 0.2636, 0.0429, 0.1977],\n",
       "        [0.2722, 0.1591, 0.2811, 0.0847, 0.2028],\n",
       "        [0.1406, 0.4477, 0.1324, 0.1323, 0.1470],\n",
       "        [0.3013, 0.0590, 0.3992, 0.1675, 0.0729],\n",
       "        [0.1256, 0.1801, 0.2037, 0.1301, 0.3605],\n",
       "        [0.1102, 0.1600, 0.2069, 0.2978, 0.2252],\n",
       "        [0.2652, 0.3208, 0.1720, 0.0888, 0.1532],\n",
       "        [0.0899, 0.1748, 0.1472, 0.4540, 0.1341],\n",
       "        [0.2273, 0.1294, 0.1712, 0.1687, 0.3034],\n",
       "        [0.5120, 0.0680, 0.2164, 0.1094, 0.0942],\n",
       "        [0.2237, 0.1700, 0.1595, 0.3483, 0.0985],\n",
       "        [0.1783, 0.1284, 0.3604, 0.2023, 0.1306],\n",
       "        [0.2033, 0.1257, 0.2696, 0.1012, 0.3002],\n",
       "        [0.1300, 0.3022, 0.1958, 0.2161, 0.1560],\n",
       "        [0.3020, 0.0563, 0.1150, 0.3588, 0.1679],\n",
       "        [0.3599, 0.0805, 0.1395, 0.1085, 0.3117],\n",
       "        [0.1981, 0.0619, 0.1815, 0.3464, 0.2121],\n",
       "        [0.2875, 0.2512, 0.2411, 0.0549, 0.1652],\n",
       "        [0.1908, 0.1227, 0.2145, 0.2915, 0.1805],\n",
       "        [0.1548, 0.2913, 0.2415, 0.1743, 0.1382],\n",
       "        [0.1888, 0.2601, 0.2689, 0.1262, 0.1560],\n",
       "        [0.0791, 0.3214, 0.1536, 0.1976, 0.2483],\n",
       "        [0.3761, 0.2307, 0.1621, 0.0778, 0.1533],\n",
       "        [0.2478, 0.2048, 0.2155, 0.1633, 0.1686],\n",
       "        [0.2004, 0.2107, 0.3965, 0.1037, 0.0887],\n",
       "        [0.2650, 0.1377, 0.2127, 0.2430, 0.1416],\n",
       "        [0.1489, 0.1787, 0.2834, 0.1536, 0.2355],\n",
       "        [0.0955, 0.0559, 0.2215, 0.4259, 0.2011],\n",
       "        [0.3775, 0.3329, 0.1156, 0.0660, 0.1080],\n",
       "        [0.1230, 0.2459, 0.1865, 0.2471, 0.1974],\n",
       "        [0.1407, 0.1867, 0.2308, 0.3513, 0.0905],\n",
       "        [0.1406, 0.1932, 0.2769, 0.2907, 0.0987],\n",
       "        [0.1944, 0.2436, 0.1338, 0.2779, 0.1503],\n",
       "        [0.1444, 0.2196, 0.1630, 0.2324, 0.2406],\n",
       "        [0.2333, 0.4467, 0.0604, 0.0434, 0.2162],\n",
       "        [0.2293, 0.1712, 0.2115, 0.1402, 0.2479],\n",
       "        [0.2074, 0.3209, 0.2275, 0.1114, 0.1327],\n",
       "        [0.2252, 0.0943, 0.3555, 0.1820, 0.1431],\n",
       "        [0.0758, 0.1762, 0.3045, 0.3929, 0.0505],\n",
       "        [0.2932, 0.2178, 0.1605, 0.1785, 0.1501],\n",
       "        [0.1522, 0.1002, 0.3066, 0.3148, 0.1262],\n",
       "        [0.0953, 0.2512, 0.1900, 0.2373, 0.2263],\n",
       "        [0.1101, 0.2470, 0.1825, 0.1836, 0.2769],\n",
       "        [0.1513, 0.1163, 0.2155, 0.3626, 0.1544],\n",
       "        [0.1157, 0.0854, 0.3720, 0.1694, 0.2574],\n",
       "        [0.1587, 0.3269, 0.1617, 0.1369, 0.2159],\n",
       "        [0.3252, 0.0481, 0.1686, 0.3265, 0.1315],\n",
       "        [0.1744, 0.1102, 0.2510, 0.1715, 0.2928],\n",
       "        [0.2137, 0.2083, 0.2825, 0.1393, 0.1562],\n",
       "        [0.3537, 0.1376, 0.1117, 0.0545, 0.3425],\n",
       "        [0.1314, 0.2558, 0.1298, 0.1935, 0.2895],\n",
       "        [0.2471, 0.0594, 0.2244, 0.3825, 0.0866],\n",
       "        [0.1851, 0.2819, 0.2396, 0.2421, 0.0514],\n",
       "        [0.1256, 0.0907, 0.3215, 0.2036, 0.2587],\n",
       "        [0.2026, 0.2655, 0.1381, 0.1976, 0.1962],\n",
       "        [0.1758, 0.1837, 0.2689, 0.2572, 0.1144],\n",
       "        [0.2890, 0.1120, 0.1170, 0.3165, 0.1655],\n",
       "        [0.1421, 0.1779, 0.2383, 0.3409, 0.1008],\n",
       "        [0.1723, 0.2054, 0.1951, 0.2346, 0.1926],\n",
       "        [0.2737, 0.1942, 0.1039, 0.0881, 0.3401],\n",
       "        [0.1990, 0.2080, 0.1920, 0.2215, 0.1796],\n",
       "        [0.0984, 0.3149, 0.1205, 0.2177, 0.2485],\n",
       "        [0.0870, 0.2583, 0.2737, 0.2959, 0.0852],\n",
       "        [0.4508, 0.1170, 0.1039, 0.1729, 0.1554],\n",
       "        [0.3195, 0.2713, 0.1476, 0.1251, 0.1365],\n",
       "        [0.1312, 0.1416, 0.1845, 0.3333, 0.2094],\n",
       "        [0.2795, 0.0889, 0.1889, 0.3660, 0.0767],\n",
       "        [0.3267, 0.0317, 0.1748, 0.3743, 0.0924],\n",
       "        [0.3019, 0.0770, 0.3183, 0.1702, 0.1325],\n",
       "        [0.2639, 0.1728, 0.2091, 0.1621, 0.1921],\n",
       "        [0.1589, 0.2695, 0.2664, 0.1295, 0.1756],\n",
       "        [0.2307, 0.0901, 0.2319, 0.3073, 0.1400],\n",
       "        [0.1349, 0.4253, 0.0850, 0.1865, 0.1682],\n",
       "        [0.1043, 0.4691, 0.0560, 0.1550, 0.2156],\n",
       "        [0.2485, 0.1156, 0.1968, 0.2949, 0.1442],\n",
       "        [0.1283, 0.2812, 0.2896, 0.0760, 0.2249],\n",
       "        [0.4831, 0.1972, 0.0856, 0.0749, 0.1592],\n",
       "        [0.2351, 0.3296, 0.1922, 0.1270, 0.1161],\n",
       "        [0.1412, 0.2270, 0.2013, 0.2476, 0.1828],\n",
       "        [0.1928, 0.2053, 0.3539, 0.1209, 0.1271],\n",
       "        [0.1504, 0.2237, 0.1003, 0.1642, 0.3613],\n",
       "        [0.2649, 0.1351, 0.1386, 0.1647, 0.2967],\n",
       "        [0.2650, 0.2611, 0.1204, 0.0754, 0.2780],\n",
       "        [0.2480, 0.2940, 0.2043, 0.1439, 0.1099],\n",
       "        [0.1369, 0.2211, 0.1486, 0.1777, 0.3156],\n",
       "        [0.3883, 0.1628, 0.1820, 0.1418, 0.1251],\n",
       "        [0.2368, 0.1183, 0.0851, 0.4468, 0.1131],\n",
       "        [0.1607, 0.1539, 0.2720, 0.2556, 0.1578],\n",
       "        [0.2410, 0.0808, 0.1736, 0.2784, 0.2262],\n",
       "        [0.2932, 0.0548, 0.1701, 0.2736, 0.2082],\n",
       "        [0.1765, 0.1663, 0.1593, 0.2379, 0.2599],\n",
       "        [0.2495, 0.3766, 0.1187, 0.0945, 0.1607],\n",
       "        [0.0512, 0.1466, 0.3145, 0.2630, 0.2247],\n",
       "        [0.3684, 0.1522, 0.0752, 0.0636, 0.3405],\n",
       "        [0.1697, 0.3584, 0.2212, 0.1153, 0.1354],\n",
       "        [0.1430, 0.2619, 0.2218, 0.2328, 0.1405]], device='cuda:0',\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "\n",
    "model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14493,     0,     0,     0,     0],\n",
       "       [ 1176,     0,     0,     0,     0],\n",
       "       [ 3813,     0,     0,     0,     0],\n",
       "       [  548,     0,     0,     0,     0],\n",
       "       [ 1454,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matr = confusion_matrix(target, pred)\n",
    "conf_matr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14493,     0,     0,     0,     0],\n",
       "       [ 1176,     0,     0,     0,     0],\n",
       "       [ 3813,     0,     0,     0,     0],\n",
       "       [  548,     0,     0,     0,     0],\n",
       "       [ 1454,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_matr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(confusion_matrices, class_mapping):\n",
    "    n_classes = len(class_mapping)\n",
    "    \n",
    "    # Initialize arrays to store metrics\n",
    "    precision = np.zeros((len(confusion_matrices), n_classes))\n",
    "    recall = np.zeros((len(confusion_matrices), n_classes))\n",
    "    f1_score = np.zeros((len(confusion_matrices), n_classes))\n",
    "    \n",
    "    for i, cm in enumerate(confusion_matrices):\n",
    "        for class_name, class_index in class_mapping.items():\n",
    "            tp = cm[class_index, class_index]\n",
    "            fp = np.sum(cm[:, class_index]) - tp\n",
    "            fn = np.sum(cm[class_index, :]) - tp\n",
    "            \n",
    "            # Calculate precision\n",
    "            precision[i, class_index] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "            \n",
    "            # Calculate recall\n",
    "            recall[i, class_index] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "            \n",
    "            # Calculate F1-score\n",
    "            f1_score[i, class_index] = 2 * (precision[i, class_index] * recall[i, class_index]) / (precision[i, class_index] + recall[i, class_index]) if (precision[i, class_index] + recall[i, class_index]) > 0 else 0\n",
    "    \n",
    "    # Calculate mean and standard deviation for each metric\n",
    "    mean_precision = np.mean(precision, axis=0) * 100\n",
    "    std_precision = np.std(precision, axis=0) * 100\n",
    "    mean_recall = np.mean(recall, axis=0) * 100\n",
    "    std_recall = np.std(recall, axis=0) * 100\n",
    "    mean_f1 = np.mean(f1_score, axis=0) * 100\n",
    "    std_f1 = np.std(f1_score, axis=0) * 100\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Class\\t\\tPrecision\\t\\tRecall\\t\\t\\tF1-Score\")\n",
    "    print(\"-\" * 70)\n",
    "    for class_name, class_index in class_mapping.items():\n",
    "        print(f\"{class_name}\\t\\t{mean_precision[class_index]:.2f}±{std_precision[class_index]:.2f}\\t\\t{mean_recall[class_index]:.2f}±{std_recall[class_index]:.2f}\\t\\t{mean_f1[class_index]:.2f}±{std_f1[class_index]:.2f}\")\n",
    "\n",
    "    \n",
    "class_mapping = {\n",
    "    \"Awake\": 4,\n",
    "    \"NREM1\": 0,\n",
    "    \"NREM2\": 1,\n",
    "    \"NREM3\": 2,\n",
    "    \"REM\": 3\n",
    "}\n",
    "\n",
    "class_mapping"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
